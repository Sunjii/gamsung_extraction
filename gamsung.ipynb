{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "  \"\"\" Dataset êµ¬ì„±ì„ ìœ„í•œ class.\"\"\"\n",
    "  def __init__(self, pair_dataset, labels):\n",
    "    self.pair_dataset = pair_dataset\n",
    "    self.labels = labels\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
    "    item['labels'] = torch.tensor(self.labels[idx])\n",
    "    return item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "def preprocessing_dataset(dataset):\n",
    "    \"\"\"ì²˜ìŒ ë¶ˆëŸ¬ì˜¨ csv íŒŒì¼ì„ ì›í•˜ëŠ” í˜•íƒœì˜ DataFrameìœ¼ë¡œ ë³€ê²½\"\"\"\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for i,j in zip(dataset['label'], dataset['sentence']):\n",
    "        labels.append(i)\n",
    "        sentences.append(j)\n",
    "    out_dataset = pd.DataFrame({'label':dataset['label'], 'sentence':dataset['sentence']})\n",
    "    return out_dataset\n",
    "\n",
    "def load_data(dataset_dir):\n",
    "    \"\"\"csv íŒŒì¼ì„ ê²½ë¡œì— ë§¡ê²Œ ë¶ˆëŸ¬ ì˜µë‹ˆë‹¤.\"\"\"\n",
    "    pd_dataset = pd.read_csv(dataset_dir, encoding='cp949')\n",
    "    dataset = preprocessing_dataset(pd_dataset)\n",
    "    return dataset\n",
    "\n",
    "def tokenized_dataset(dataset, tokenizer):\n",
    "  \"\"\" tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤.\"\"\"\n",
    "  concat_entity = []\n",
    "  for s in dataset['sentence']: # ë°ì´í„°ì…‹ì˜ ì—”í‹°í‹° ë‘ê°œë¥¼ ì¶”ì¶œ\n",
    "    concat_entity.append(s)\n",
    "  tokenized_sentences = tokenizer(\n",
    "      concat_entity,\n",
    "      #list(dataset['sentence']),\n",
    "      return_tensors=\"pt\",\n",
    "      padding=True,\n",
    "      truncation=True,\n",
    "      max_length=256,\n",
    "      add_special_tokens=True,\n",
    "      )\n",
    "  return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 3 4 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_list = ['ë¶„ë…¸', 'ìŠ¬í””','ë¶ˆì•ˆ','ìƒì²˜','ë‹¹í™©','ê¸°ì¨']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(label_list)\n",
    "\n",
    "print(encoder.transform(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_num(label):\n",
    "    num_label = encoder.transform(label)\n",
    "    return num_label\n",
    "\n",
    "def num_to_label(label):\n",
    "    origin_label = encoder.inverse_transform(label)\n",
    "    return origin_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "def klue_re_micro_f1(preds, labels):\n",
    "    \"\"\"KLUE-RE micro f1 (except no_relation)\"\"\"\n",
    "    label_list = ['ë¶„ë…¸', 'ìŠ¬í””','ë¶ˆì•ˆ','ìƒì²˜','ë‹¹í™©','ê¸°ì¨']\n",
    "    #no_relation_label_idx = label_list.index(\"no_relation\")\n",
    "    label_indices = list(range(len(label_list)))\n",
    "    #label_indices.remove(no_relation_label_idx)\n",
    "    return sklearn.metrics.f1_score(labels, preds, average=\"micro\", labels=label_indices) * 100.0\n",
    "\n",
    "def klue_re_auprc(probs, labels):\n",
    "    \"\"\"KLUE-RE AUPRC (with no_relation)\"\"\"\n",
    "    labels = np.eye(6)[labels]\n",
    "\n",
    "    score = np.zeros((6,))\n",
    "    for c in range(6):\n",
    "        targets_c = labels.take([c], axis=1).ravel()\n",
    "        preds_c = probs.take([c], axis=1).ravel()\n",
    "        precision, recall, _ = sklearn.metrics.precision_recall_curve(\n",
    "            targets_c, preds_c)\n",
    "        score[c] = sklearn.metrics.auc(recall, precision)\n",
    "    return np.average(score) * 100.0\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\" validationì„ ìœ„í•œ metrics function \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    probs = pred.predictions\n",
    "\n",
    "    # calculate accuracy using sklearn's function\n",
    "    f1 = klue_re_micro_f1(preds, labels)\n",
    "    auprc = klue_re_auprc(probs, labels)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "\n",
    "    return {\n",
    "        'micro f1 score': f1,\n",
    "        'auprc': auprc,\n",
    "        'accuracy': acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                           sentence\n",
      "0        ê¸°ì¨                          ì•„ë‚´ê°€ ë“œë””ì–´ ì¶œì‚°í•˜ê²Œ ë˜ì–´ì„œ ì •ë§ ì‹ ì´ ë‚˜.\n",
      "1        ë¶ˆì•ˆ            ë‹¹ë‡¨ë‘ í•©ë³‘ì¦ ë•Œë¬¸ì— ë¨¹ì–´ì•¼ í•  ì•½ì´ ì—´ ê°€ì§€ê°€ ë„˜ì–´ê°€ë‹ˆê¹Œ ìŠ¤íŠ¸ë ˆìŠ¤ì•¼.\n",
      "2        ë‹¹í™©            ê³ ë“±í•™êµì— ì˜¬ë¼ì˜¤ë‹ˆ ì¤‘í•™êµ ë•Œë³´ë‹¤ ìˆ˜ì—…ì´ ê°‘ìê¸° ì–´ë ¤ì›Œì ¸ì„œ ë‹¹í™©ìŠ¤ëŸ¬ì›Œ.\n",
      "3        ê¸°ì¨      ì¬ì·¨ì—…ì´ ë¼ì„œ ë°›ê²Œ ëœ ì²« ì›”ê¸‰ìœ¼ë¡œ ì˜¨ ê°€ì¡±ì´ ì™¸ì‹ì„ í•  ì˜ˆì •ì´ì•¼. ë„ˆë¬´ í–‰ë³µí•´.\n",
      "4        ê¸°ì¨                       ë¹šì„ ë“œë””ì–´ ë‹¤ ê°šê²Œ ë˜ì–´ì„œ ì´ì œì•¼ ì•ˆë„ê°ì´ ë“¤ì–´.\n",
      "...     ...                                                ...\n",
      "40874    ë¶ˆì•ˆ  ê°™ì´ ì‚¬ëŠ” ì¹œêµ¬ê°€ ì• ì™„ê²¬ì„ ë°ë ¤ì™”ëŠ”ë° ëŒ€ë¶€ë¶„ ë‚´ê°€ ëŒë³´ê³  ìˆì–´. ë‚´ê°€ ì£¼ì¸ì¸ê°€ í˜¼ë€...\n",
      "40875    ê¸°ì¨                  ì§€ë‚œì£¼ì— ê±´ê°•ê²€ì§„ ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ë° ì •ìƒì´ë¼ê³  ê²°ê³¼ê°€ ë‚˜ì™”ì–´.\n",
      "40876    ìŠ¬í””          ì—„ë§ˆëŠ” ë‚´ ê¿ˆì¸ ì‘ê°€ë¥¼ ì‘ì›í•´ ì£¼ê³ ëŠ” í–ˆëŠ”ë° ì§€ê¸ˆì€ ì•ˆ ê·¸ë˜. ë„ˆë¬´ ìŠ¬í¼.\n",
      "40877    ê¸°ì¨            ì´ë ‡ê²Œ ì¢‹ì€ ìš´ë™ ì‹œì„¤ì—ì„œ ê²½ë¡œ ìš°ëŒ€ë¡œ ìš´ë™í•  ìˆ˜ ìˆë‹¤ë‹ˆ ì°¸ í–‰ìš´ì´ì•¼.\n",
      "40878    ë¶ˆì•ˆ                ì¹œêµ¬ ê´€ê³„ê°€ ë„ˆë¬´ í˜ë“¤ì–´. ë² í‘¸ëŠ” ë§Œí¼ ëŒì•„ì˜¤ì§€ ì•ŠëŠ” ê²ƒ ê°™ì•„.\n",
      "\n",
      "[40879 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_data('./dataset.csv')\n",
    "dataset[\"label\"]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train():\n",
    "    seed_everything(42)\n",
    "    # load model and tokenizer\n",
    "    MODEL_NAME = 'klue/bert-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # load dataset\n",
    "    dataset = load_data('./dataset.csv')\n",
    "    target = label_to_num(dataset[\"label\"].values)\n",
    "    train_dataset, dev_dataset = train_test_split(\n",
    "        dataset, test_size=0.15, shuffle=True, stratify=target,\n",
    "    )\n",
    "\n",
    "    train_label = label_to_num(train_dataset['label'].values)\n",
    "    dev_label = label_to_num(dev_dataset['label'].values)\n",
    "\n",
    "    # tokenizing dataset\n",
    "    tokenized_train = tokenized_dataset(train_dataset, tokenizer)\n",
    "    tokenized_dev = tokenized_dataset(dev_dataset, tokenizer)\n",
    "\n",
    "    # make dataset for pytorch.\n",
    "    RE_train_dataset = RE_Dataset(tokenized_train, train_label)\n",
    "    RE_dev_dataset = RE_Dataset(tokenized_dev, dev_label)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    # setting model hyperparameter\n",
    "    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "    model_config.num_labels = 6\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, config=model_config)\n",
    "    print(model.config)\n",
    "    model.parameters\n",
    "    model.to(device)\n",
    "\n",
    "    # ì‚¬ìš©í•œ option ì™¸ì—ë„ ë‹¤ì–‘í•œ optionë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        save_total_limit=5,              # number of total save model.\n",
    "        save_steps=500,                 # model saving step.\n",
    "        num_train_epochs=10,              # total number of training epochs\n",
    "        learning_rate=1e-4,               # learning_rate\n",
    "        # batch size per device during training\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "        warmup_ratio=0.1,\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=100,              # log saving step.\n",
    "        evaluation_strategy='steps',  # evaluation strategy to adopt during training\n",
    "        # `no`: No evaluation during training.\n",
    "        # `steps`: Evaluate every `eval_steps`.\n",
    "        # `epoch`: Evaluate every end of epoch.\n",
    "        eval_steps=500,            # evaluation step.\n",
    "        metric_for_best_model='eval_micro f1 score',  # eval_micro f1 score\n",
    "        load_best_model_at_end=True,\n",
    "        lr_scheduler_type='cosine',  # default: linear\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        # the instantiated ğŸ¤— Transformers model to be trained\n",
    "        model=model,\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=RE_train_dataset,         # training dataset\n",
    "        eval_dataset=RE_dev_dataset,             # evaluation dataset\n",
    "        compute_metrics=compute_metrics         # define metrics function\n",
    "    )\n",
    "\n",
    "    # trainer.hyperparameter_search(direction=\"maximize\", hp_space=my_hp_space_ray)\n",
    "    # train model\n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(f'./best_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda empty cache!!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"cuda empty cache!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.5.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5430' max='5430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5430/5430 1:03:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Micro f1 score</th>\n",
       "      <th>Auprc</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.032300</td>\n",
       "      <td>1.075343</td>\n",
       "      <td>61.692759</td>\n",
       "      <td>68.550034</td>\n",
       "      <td>0.616928</td>\n",
       "      <td>10.289300</td>\n",
       "      <td>595.956000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.937700</td>\n",
       "      <td>1.003533</td>\n",
       "      <td>64.155251</td>\n",
       "      <td>71.447631</td>\n",
       "      <td>0.641553</td>\n",
       "      <td>10.454600</td>\n",
       "      <td>586.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.761100</td>\n",
       "      <td>1.050495</td>\n",
       "      <td>64.171559</td>\n",
       "      <td>70.751000</td>\n",
       "      <td>0.641716</td>\n",
       "      <td>10.288800</td>\n",
       "      <td>595.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.547900</td>\n",
       "      <td>1.149174</td>\n",
       "      <td>65.410959</td>\n",
       "      <td>69.971622</td>\n",
       "      <td>0.654110</td>\n",
       "      <td>10.279100</td>\n",
       "      <td>596.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.344000</td>\n",
       "      <td>1.339058</td>\n",
       "      <td>64.856491</td>\n",
       "      <td>69.484260</td>\n",
       "      <td>0.648565</td>\n",
       "      <td>10.282400</td>\n",
       "      <td>596.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.212800</td>\n",
       "      <td>1.508211</td>\n",
       "      <td>65.117417</td>\n",
       "      <td>68.712111</td>\n",
       "      <td>0.651174</td>\n",
       "      <td>10.275000</td>\n",
       "      <td>596.789000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>1.749596</td>\n",
       "      <td>65.231572</td>\n",
       "      <td>68.146820</td>\n",
       "      <td>0.652316</td>\n",
       "      <td>10.421700</td>\n",
       "      <td>588.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>1.938252</td>\n",
       "      <td>64.774951</td>\n",
       "      <td>67.704602</td>\n",
       "      <td>0.647750</td>\n",
       "      <td>11.133000</td>\n",
       "      <td>550.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.045200</td>\n",
       "      <td>2.046523</td>\n",
       "      <td>65.362035</td>\n",
       "      <td>67.753900</td>\n",
       "      <td>0.653620</td>\n",
       "      <td>10.334100</td>\n",
       "      <td>593.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>2.097542</td>\n",
       "      <td>65.215264</td>\n",
       "      <td>67.841107</td>\n",
       "      <td>0.652153</td>\n",
       "      <td>11.554000</td>\n",
       "      <td>530.727000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-kê°œ ì¶”ë¡  ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì\n",
    "\n",
    "def tokenizing(sentence, tokenizer):\n",
    "    \"\"\" tokenizerì— ë”°ë¼ sentenceë¥¼ tokenizing í•©ë‹ˆë‹¤.\"\"\"\n",
    "    tokenized_sentence = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        add_special_tokens=True,\n",
    "      )\n",
    "        \n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "a = tokenizing(\"ì•ˆë…•í•˜ì„¸ìš”.\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2, 5891, 2205, 5971,   18,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[   2, 5891, 2205, 5971,   18,    3]])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(a['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def inference(sentence, model):\n",
    "    model.eval()\n",
    "    MODEL_NAME = 'klue/bert-base'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    output_pred = []\n",
    "    output_prob = []\n",
    "    \n",
    "    # sentence í† í¬ë‚˜ì´ì§•\n",
    "    tokenized_sent = tokenizing(sentence, tokenizer)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=tokenized_sent['input_ids'].to(device),\n",
    "            attention_mask=tokenized_sent['attention_mask'].to(device),\n",
    "            token_type_ids=tokenized_sent['token_type_ids'].to(device)\n",
    "        )\n",
    "    '''\n",
    "    SequenceClassifierOutput(loss=None, logits=tensor([[ 2.8013,  0.9778, -2.0565,  2.7867, -1.6169, -2.9289]],\n",
    "       device='cuda:0'), hidden_states=None, attentions=None)\n",
    "    '''\n",
    "    \n",
    "    logits = outputs[0]\n",
    "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    result = np.argmax(logits, axis=-1)    \n",
    "    \n",
    "    output_pred.append(result)\n",
    "    output_prob.append(prob)\n",
    "  \n",
    "    return np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentence):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('./best_model')\n",
    "    model.to(device)\n",
    "\n",
    "    pred_answer, output_prob = inference(sentence, model)\n",
    "    pred_answer = num_to_label(pred_answer)\n",
    "\n",
    "    print(pred_answer, output_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ê¸°ì¨'] [[0.46331310272216797, 0.008690078742802143, 0.05478760227560997, 0.11694328486919403, 0.034610211849212646, 0.32165566086769104]]\n",
      "['ë¶ˆì•ˆ'] [[0.007257037330418825, 0.002930098446086049, 0.030506860464811325, 0.5612196922302246, 0.022893842309713364, 0.3751924932003021]]\n",
      "['ë‹¹í™©'] [[0.0023562866263091564, 0.992356538772583, 0.0004910796997137368, 0.004630960524082184, 9.798115934245288e-05, 6.713408947689459e-05]]\n",
      "['ë‹¹í™©'] [[0.00031427424983121455, 0.9889928698539734, 0.009809978306293488, 0.0005756760365329683, 0.00015612594143021852, 0.00015096743300091475]]\n",
      "['ë¶„ë…¸'] [[0.0005168858915567398, 0.0013337378622964025, 0.7627708911895752, 0.0010220204712823033, 0.2239847034215927, 0.010371852666139603]]\n"
     ]
    }
   ],
   "source": [
    "prediction('ëˆ„êµ°ì§€ ë³´ë ¤ê³  ë‹¬ë ¤ê°„ ê³µì£¼ê°€ ë¬¸ì„ ì—´ì—ˆì„ ë•Œ ì–´ë§ˆë‚˜ ì„¸ìƒì— ë¬¸ ì•ì— ì–´ì œ ê·¸ ê°œêµ¬ë¦¬ê°€ ì•‰ì•„ ìˆì§€ ë­ì˜ˆìš”.')\n",
    "prediction('ëˆ„ê°€ ë´ë„ ê³µì£¼ì˜ ì–¼êµ´ì´ ë¶‰ìœ¼ë½í‘¸ë¥´ë½í•œ ê²Œ ë³´ì˜€ì–´ìš”.')\n",
    "prediction('ê·¸ë˜ì„œ ê³µì£¼ê°€ ë¬¸ì„ ì‚¬ì •ì—†ì´ ë‹«ì•„ë²„ë¦¬ê³ ëŠ” í—ˆë‘¥ëŒ€ë©° ë‹¤ì‹œ ìë¦¬ë¡œ ì™€ ì‹ì‚¬ë¥¼ ê³„ì†í–ˆì§€ë§Œ, ê³µì£¼ë„ ë„ˆë¬´ë‚˜ë„ ë†€ë€ ê±´ ì‚¬ì‹¤ì´ì—ˆì–´ìš”.')\n",
    "prediction('ë”¸ì• ì•¼, ì™œ ê·¸ëŸ¬ë‹ˆ? ë¬¸ ì•ì—ì„œ ë„ˆë¥¼ ë°ë ¤ê°€ë ¤ëŠ” ê±°ì¸ì´ë¼ë„ ë³¸ ê²Œëƒ?')\n",
    "prediction('â€œì•„, ì•„ëƒ,â€ë¼ë©° ê³µì£¼ê°€ ë§í–ˆì–´ìš”. â€œê±°ì¸ì´ ì•„ë‹ˆë¼ êµ¬ì—­ì§ˆë‚˜ëŠ” ê°œêµ¬ë¦¬ í•œ ë§ˆë¦¬ì•¼.â€')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ìƒì²˜'] [[0.0005434234044514596, 0.00031408053473569453, 7.354172703344375e-05, 0.0005094616208225489, 0.9984968900680542, 6.258022040128708e-05]]\n",
      "['ê¸°ì¨'] [[0.9994732737541199, 8.953684300649911e-05, 0.00020760462211910635, 8.277779852505773e-05, 4.648189860745333e-05, 0.0001003616489470005]]\n",
      "['ê¸°ì¨'] [[0.9990330934524536, 8.229914237745106e-05, 0.00012298690853640437, 0.0001815058058127761, 7.205220026662573e-05, 0.0005080334958620369]]\n"
     ]
    }
   ],
   "source": [
    "span1 = 'ê±°ëŒ€í•œ ìˆ²ì—ì„œ ì•„ë‚´ì™€ í•¨ê»˜ í˜ë“¤ê²Œ ì‚´ê³  ìˆëŠ” ë‚˜ë¬´ê¾¼ì´ í•˜ë‚˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì—ê²ŒëŠ” ì„¸ ì‚´ ë°°ê¸° ì–´ë¦° ë”¸ì´ í•˜ë‚˜ ìˆì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ë“¤ì€ ë„ˆë¬´ë„ ê°€ë‚œí–ˆê¸° ë•Œë¬¸ì— ì–‘ì‹ì„ êµ¬í•  ê¸¸ì´ ì—†ì–´ ë”¸ì•„ì´ë¥¼ ì–´ì°Œ ë¨¹ì—¬ì‚´ë¦´ì§€ ì•ì´ ê¹œê¹œí–ˆìŠµë‹ˆë‹¤.'\n",
    "span2 = 'ì¼ê³± ëª…ì˜ ìƒˆë¼ ì—¼ì†Œë“¤ì´ ê·¸ ê´‘ê²½ì„ ì§€ì¼œë³´ë‹¤ ìš°ë¬¼ê°€ë¡œ ë‹¬ë ¤ì™€ í™˜í˜¸ì„±ì„ ì§€ë¥´ë©° ì†Œë¦¬ì³¤ì–´ìš”. â€œë§Œì„¸! ëŠ‘ëŒ€ê°€ ì£½ì—ˆë‹¤! ëŠ‘ëŒ€ê°€ ì£½ì—ˆì–´!â€ ìƒˆë¼ ì—¼ì†Œë“¤ì€ ë„ˆë¬´ ê¸°ë»ì„œ ì—„ë§ˆ ì—¼ì†Œì™€ í•¨ê»˜ ìš°ë¬¼ê°€ ì£¼ë³€ì„ ë‘¥ê¸€ê²Œ ëŒë©° ì¶¤ì„ ì¶”ì—ˆë‹µë‹ˆë‹¤.'\n",
    "span3 = 'ì™•ì€ ì–´ì©ì§€ ì œë¹„ê°€ ë‘ë ¤ì› ë‹¤. ìì‹ ì²˜ëŸ¼ ì‘ì€ ì´ë“¤ì—ê²ŒëŠ” ê±°êµ¬ì˜ ìƒˆì²˜ëŸ¼ ë³´ì˜€ë‹¤. í•˜ì§€ë§Œ ì—„ì§€ ê³µì£¼ë¥¼ ë³´ê³ ëŠ” ë¬´ì²™ ê¸°ë»í–ˆë‹¤. ì†Œë…€ëŠ” ì§€ê¸ˆê» ë³¸ ê°€ì¥ ì•„ë¦„ë‹¤ìš´ ì†Œë…€ì˜€ë‹¤. ê·¸ë˜ì„œ í™©ê¸ˆ ì™•ê´€ì„ ë²—ì–´ ì†Œë…€ì˜ ë¨¸ë¦¬ì— ì–¹ê³ ëŠ” ì´ë¦„ì„ ë¬¼ì–´ë„ ë˜ëƒê³  ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ë¬»ê³ ëŠ” ìì‹ ì˜ ì•„ë‚´ê°€ ë˜ì–´ ë‹¬ë¼ê³  ë¶€íƒí–ˆë‹¤. ê·¸ëŸ¬ë©´ ì†Œë…€ëŠ” ëª¨ë“  ê½ƒì˜ ì—¬ì™•ì´ ë  ê²ƒì´ë‹¤. ì •ë§ì´ì§€ ë‘êº¼ë¹„ ì•„ë“¤, ê²€ì€ ë²¨ë²³ ì½”íŠ¸ë¥¼ ì…ì€ ë‘ë”ì§€ì™€ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ë‚¨í¸ê°ì´ì—ˆë‹¤. ê·¸ë˜ì„œ ì†Œë…€ëŠ” ì´ ë§¤ë ¥ì ì¸ ì™•ì—ê²Œ â€œì¢‹ì•„ìš”â€ë¼ê³  ë§í–ˆë‹¤. ê½ƒ ì†ì—ì„œ ìê·¸ë§ˆí•œ ìˆ™ë…€ì™€ ì‹ ì‚¬ë“¤ì´ ë‚˜ì™€ ì¦ê²ê²Œ ì§€ì¼œë³´ì•˜ë‹¤. ê°ì ì—„ì§€ ê³µì£¼ì—ê²Œ ì„ ë¬¼ì„ ì£¼ì—ˆëŠ”ë° ìµœê³ ì˜ ì„ ë¬¼ì€ ì»¤ë‹¤ë€ ì€ìƒ‰ íŒŒë¦¬ê°€ ë‹¬ì•˜ë˜ ë‚ ê°œ í•œ ìŒì´ì—ˆë‹¤. ê·¸ ë‚ ê°œë¥¼ ë“±ì— ë‹¨ë‹¨íˆ ë¬¶ì ì—„ì§€ ê³µì£¼ë„ ê½ƒê³¼ ê½ƒ ì‚¬ì´ë¥¼ ì‚´ë‘ì‚´ë‘ ëŒì•„ë‹¤ë‹ ìˆ˜ ìˆì—ˆë‹¤. ëª¨ë‘ê°€ ì¦ê±°ì›Œí–ˆë‹¤. ì œë¹„ëŠ” ì´ë“¤ ìœ„, ìê¸° ë‘¥ì§€ì— ìë¦¬ë¥¼ ì¡ê³ ëŠ” ì œì¼ ì˜í•˜ëŠ” ë…¸ë˜ë¥¼ ë“¤ë ¤ì£¼ì—ˆë‹¤. ê·¸ë ‡ì§€ë§Œ ë§ˆìŒ ê¹Šì€ ê³³ì—ì„œëŠ” ìŠ¬í””ì´ ë°€ë ¤ì™”ë‹¤. ì—„ì§€ ê³µì£¼ë¥¼ ë¬´ì²™ì´ë‚˜ ì¢‹ì•„í•´ì„œ ì ˆëŒ€ í—¤ì–´ì§€ê³  ì‹¶ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì´ë‹¤.'\n",
    "\n",
    "\n",
    "prediction(span1)\n",
    "prediction(span2)\n",
    "prediction(span3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
